# -*- coding: utf-8 -*-
"""RAGAS-evaluate-replicates.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t6SqKim6sseYNvgRxb33PbjpGtT2Ikuy
"""

!pip install ragas langchain langchain-core datasets evaluate scipy requests

import os
from getpass import getpass

os.environ["GROQ_API_KEY"] = getpass("Enter your GROQ API Key:")

from langchain_core.language_models import LLM
from langchain_core.outputs import LLMResult, Generation
from typing import List, Optional
import requests

class GroqLLM(LLM):
    model: str = "llama3-8b-8192"
    temperature: float = 0.0
    max_tokens: int = 512

    def _generate(self, prompts: List[str], stop: Optional[List[str]] = None, **kwargs) -> LLMResult:
        generations = []
        for prompt in prompts:
            headers = {
                "Authorization": f"Bearer {os.environ['GROQ_API_KEY']}",
                "Content-Type": "application/json"
            }
            body = {
                "messages": [{"role": "user", "content": prompt}],
                "model": self.model,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
            }

            response = requests.post("https://api.groq.com/openai/v1/chat/completions", headers=headers, json=body)

            try:
                response.raise_for_status()
            except requests.exceptions.HTTPError as e:
                print(" Groq API Error:", response.status_code)
                print(" Response:", response.text)
                raise e

            content = response.json()["choices"][0]["message"]["content"].strip()
            generations.append([Generation(text=content)])

        return LLMResult(generations=generations)

    def _call(self, prompt: str, stop: Optional[List[str]] = None, run_manager=None, **kwargs) -> str:
        result = self._generate([prompt], stop=stop, **kwargs)
        return result.generations[0][0].text

    @property
    def _llm_type(self) -> str:
        return "groq"

from ragas.llms import LangchainLLMWrapper

evaluator_llm = LangchainLLMWrapper(GroqLLM())

from ragas import EvaluationDataset

mock_dataset = EvaluationDataset.from_list([
    {
        "user_input": "What is the capital of France?",
        "retrieved_contexts": ["Paris is the capital of France."],
        "response": "Paris",
        "reference": "Paris"
    }
])

from ragas import evaluate
import numpy as np
from scipy import stats

def extract_float(v):
    # Recursively go deeper until we find a float
    while isinstance(v, dict):
        v = list(v.values())[0]
    return float(v)

def evaluate_with_replicates(dataset, metrics, llm=None, num_replicates: int = 3):
    replicate_results = []

    for _ in range(num_replicates):
        result = evaluate(dataset, metrics=metrics, llm=llm)
        scores = result.scores

        if isinstance(scores, list):
            scores = scores[0]  # Take the first item
        if isinstance(scores, dict):
            scores = {"score": extract_float(scores)}
        else:
            scores = {"score": float(scores)}

        replicate_results.append(scores)

    summary = {}
    for key in replicate_results[0].keys():
        values = [r[key] for r in replicate_results]
        mean = np.mean(values)
        std = np.std(values, ddof=1) if len(values) > 1 else 0.0
        median = np.median(values)
        min_val = np.min(values)
        max_val = np.max(values)

        if len(values) > 1 and std > 0:
            ci_low, ci_high = stats.t.interval(
                0.95, len(values) - 1, loc=mean, scale=stats.sem(values)
            )
        else:
            ci_low = ci_high = mean

        summary[key] = {
            "mean": mean,
            "std": std,
            "median": median,
            "min": min_val,
            "max": max_val,
            "ci_95": [ci_low, ci_high],
        }

    return summary

from ragas.metrics import faithfulness
import pprint

result = evaluate_with_replicates(
    dataset=mock_dataset,
    metrics=[faithfulness],
    llm=evaluator_llm,
    num_replicates=3
)

pprint.pprint(result)

